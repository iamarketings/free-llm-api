# ğŸš€ Free LLM API â€” Dynamic Proxy

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Node.js Version](https://img.shields.io/badge/node-%3E%3D18-green.svg)](https://nodejs.org/)
[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-blue.svg)](https://github.com/iamarketings/free-llm-api/graphs/commit-activity)

> ğŸ‡«ğŸ‡· [Lire en FranÃ§ais](#-version-franÃ§aise) | ğŸ‡¬ğŸ‡§ [Read in English](#-english-version)

---

## ğŸ‡«ğŸ‡· Version FranÃ§aise

Un proxy intelligent et **compatible OpenAI** qui agrÃ¨ge automatiquement tous les modÃ¨les **gratuits** d'OpenRouter. Profitez d'un accÃ¨s illimitÃ© aux LLMs avec une gestion intelligente des pannes.

### âœ¨ Points Forts

- **ğŸ”Œ Plug & Play** : EntiÃ¨rement compatible avec les SDK OpenAI (Python, JS, LangChain).
- **ğŸ¤– 31+ ModÃ¨les Gratuits** : RÃ©cupÃ©ration en temps rÃ©el des modÃ¨les disponibles sans frais.
- **ğŸ›¡ï¸ Failover Intelligent** : Si un modÃ¨le Ã©choue ou freeze, le proxy bascule automatiquement sur le suivant.
- **ğŸ“Š Dashboard Admin** : Interface web intÃ©grÃ©e pour surveiller les logs, tester les modÃ¨les et configurer le serveur.
- **âš™ï¸ Configuration Dynamique** : Modifiez le timeout, le prompt systÃ¨me ou le mode de routage sans redÃ©marrer.

### ğŸ› ï¸ Installation Rapide

**1. Cloner le projet**
```bash
git clone https://github.com/iamarketings/free-llm-api.git
cd free-llm-api
```

**2. Installer les dÃ©pendances**
```bash
npm install
```

**3. Configurer l'environnement**

Copiez le fichier exemple et renseignez votre clÃ© :
```bash
cp .env.example .env
```
```env
OPENROUTER_API_KEY=votre_cle_ici
PORT=8000
```

**4. Lancement**
```bash
# Mode Production
npm start

# Mode DÃ©veloppement (Auto-reload)
npm run dev
```

ğŸŒ AccÃ©dez au Dashboard sur : **http://localhost:8000**

### ğŸš€ Exemples d'Utilisation

**Via Python (SDK OpenAI)**
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="sk-local"  # La clÃ© locale est ignorÃ©e par le proxy
)

response = client.chat.completions.create(
    model="auto",  # Utilise le meilleur modÃ¨le gratuit disponible
    messages=[{"role": "user", "content": "Explique-moi la physique quantique."}]
)

print(response.choices[0].message.content)
```

**Via cURL**
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "messages": [{"role": "user", "content": "Salut !"}]
  }'
```

### ğŸ“‚ Structure du Code

| Dossier | RÃ´le |
|---------|------|
| ğŸ“‚ `controllers/` | Logique mÃ©tier (Chat, Admin, Dashboard) |
| ğŸ“‚ `models/` | Gestionnaire de modÃ¨les (Fetch & Test) |
| ğŸ“‚ `views/` | Interface utilisateur (EJS + Tailwind) |
| ğŸ“‚ `routes/` | DÃ©finition des points d'entrÃ©e API |
| ğŸ“‚ `state/` | Gestion de l'Ã©tat global et persistance JSON |
| ğŸ“‚ `utils/` | Logger centralisÃ© |

### âš™ï¸ ParamÃ¨tres du `config.json`

Le fichier est gÃ©nÃ©rÃ© automatiquement au premier lancement. Vous pouvez le modifier via le Dashboard :

- `mode` : `auto` (failover automatique) ou `manual` (modÃ¨le fixe).
- `system_prompt` : Instructions ajoutÃ©es automatiquement Ã  chaque requÃªte.
- `request_timeout` : Temps max (secondes) avant de passer au modÃ¨le suivant.
- `refresh_interval` : FrÃ©quence (minutes) de mise Ã  jour de la liste des modÃ¨les.

### ğŸ¤ Contribution

Les contributions sont les bienvenues ! N'hÃ©sitez pas Ã  ouvrir une **Issue** ou une **Pull Request**.

---

## ğŸ‡¬ğŸ‡§ English Version

An intelligent, **OpenAI-compatible** proxy that automatically aggregates all **free** models from OpenRouter. Enjoy unlimited access to LLMs with smart failure handling.

### âœ¨ Key Features

- **ğŸ”Œ Plug & Play**: Fully compatible with OpenAI SDKs (Python, JS, LangChain).
- **ğŸ¤– 31+ Free Models**: Real-time fetching of all available no-cost models.
- **ğŸ›¡ï¸ Smart Failover**: If a model fails or freezes, the proxy automatically switches to the next one.
- **ğŸ“Š Admin Dashboard**: Built-in web UI to monitor logs, test models and configure the server.
- **âš™ï¸ Dynamic Config**: Change timeout, system prompt or routing mode without restarting.

### ğŸ› ï¸ Quick Start

**1. Clone the repository**
```bash
git clone https://github.com/iamarketings/free-llm-api.git
cd free-llm-api
```

**2. Install dependencies**
```bash
npm install
```

**3. Set up environment**

Copy the example file and fill in your key:
```bash
cp .env.example .env
```
```env
OPENROUTER_API_KEY=your_key_here
PORT=8000
```

**4. Run**
```bash
# Production
npm start

# Development (auto-reload)
npm run dev
```

ğŸŒ Open the Dashboard at: **http://localhost:8000**

### ğŸš€ Usage Examples

**Python (OpenAI SDK)**
```python
from openai import OpenAI

client = OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="sk-local"  # local key is ignored by the proxy
)

response = client.chat.completions.create(
    model="auto",  # uses the best available free model
    messages=[{"role": "user", "content": "Explain quantum physics."}]
)

print(response.choices[0].message.content)
```

**Node.js / Fetch**
```js
const res = await fetch("http://localhost:8000/v1/chat/completions", {
  method: "POST",
  headers: { "Content-Type": "application/json", "Authorization": "Bearer local" },
  body: JSON.stringify({
    model: "auto",
    messages: [{ role: "user", content: "Hello!" }]
  })
});
const data = await res.json();
console.log(data.choices[0].message.content);
```

**cURL**
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "auto",
    "messages": [{"role": "user", "content": "Hello!"}]
  }'
```

### ğŸ“‚ Project Structure

| Folder | Role |
|--------|------|
| ğŸ“‚ `controllers/` | Business logic (Chat, Admin, Dashboard) |
| ğŸ“‚ `models/` | Model manager (Fetch & Test) |
| ğŸ“‚ `views/` | UI templates (EJS + Tailwind) |
| ğŸ“‚ `routes/` | API endpoint definitions |
| ğŸ“‚ `state/` | Global state & JSON persistence |
| ğŸ“‚ `utils/` | Centralized logger |

### ğŸ“¡ API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| `POST` | `/v1/chat/completions` | Chat completion (OpenAI format) |
| `GET` | `/v1/models` | List active models |
| `GET` | `/health` | Service health & metrics |
| `POST` | `/refresh` | Force model re-scan |
| `GET` | `/` | Admin dashboard |

### âš™ï¸ `config.json` Settings

Auto-generated on first run, editable via the Dashboard:

- `mode`: `auto` (smart failover) or `manual` (fixed model).
- `system_prompt`: Instructions automatically prepended to every request.
- `request_timeout`: Max seconds before switching to the next model.
- `refresh_interval`: Minutes between model list updates.

### ğŸ¤ Contributing

Contributions are welcome! Feel free to open an **Issue** or a **Pull Request**.

---

## ğŸ“„ Licence / License

Distributed under the **MIT License** â€” see [LICENSE](LICENSE) for details.

---

DÃ©veloppÃ© avec â¤ï¸ par [iamarketings](https://github.com/iamarketings)
